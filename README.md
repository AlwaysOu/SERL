# Supplementary Code for [SERL]

This repository contains the **supplementary code** used to implement the experiments described in the paper:
In this project, we provide the data, prompts, and running scripts for training the General QA task. 
The complete data and prompts(Summarization and Open writing) will be made available after the results are released.

> **[SERL]**   
> **Conference/Journal:** [AAAI2026]  

The implementation is based on the [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl) library by Hugging Face, which provides state-of-the-art methods for post-training foundation models using techniques such as DPO, PPO, SFT, and more.

---

## ğŸ“¦ Requirements

Before running the code, make sure you have the following dependencies installed:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ How to Run

All training scripts are located in the `scripts/` directory. Here is our running step our pipeline:

### Training with SERL

```bash
python -m SERL.examples.scripts.SERL \
```

---

## ğŸ“ Directory Structure

```
.
â”œâ”€â”€ README.md                   <- This file
â”œâ”€â”€ requirements.txt            <- Required packages
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ SERL.py             <- SERL training script
â”‚   â”‚   â””â”€â”€ ...                 <- Additional scripts
â””â”€â”€ data/                       <- Optional: Preprocessed datasets
```

---

## ğŸ“š References

Our implementation builds on top of the TRL library:

```bibtex
@misc{vonwerra2022trl,
  author = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin GallouÃ©dec},
  title = {TRL: Transformer Reinforcement Learning},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/trl}}
}
```

---

## âœ… Notes for Reproducibility

- All random seeds are fixed in the training scripts.
- We use deterministic versions of PyTorch and Transformers where possible.
- The exact version of the libraries used is listed in `requirements.txt`.

---

## ğŸ“ License

This code is released under the **Apache 2.0 License**, same as the TRL library.

---
